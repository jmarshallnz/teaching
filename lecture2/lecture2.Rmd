---
title: "Linear models 2"
author: "Jonathan Marshall"
date: ''
output:
  ioslides_presentation:
    css: jm.css
    highlight: tango
    widescreen: yes
  beamer_presentation:
    highlight: tango
    includes:
      in_header: ../header.tex
    keep_tex: yes
graphics: yes
---

```{r setup, echo=FALSE}
library(knitr)
opts_chunk$set(dev.args=list(bg='transparent'), comment="")
#print(knit_hooks$get('output'))
knit_hooks$set(output=function (x, options) {
#  cat("OPTIONS=", names(options), "\n")
  cache_path <- unlist(strsplit(options$cache.path, '/'))
#  cat("Cache_path length=", length(cache_path), "\n")
  out_format <- cache_path[length(cache_path)]
  if (out_format == "html") {
    knitr:::escape_html(x)
    x = paste(x, collapse = "\\n")
    sprintf("<div class=\"%s\"><pre class=\"knitr %s\">%s</pre></div>\\n", 
        'output', tolower(options$engine), x)
  } else {
    paste(c("\\begin{ROutput}",
            sub("\n$", "", x),
            "\\end{ROutput}",
            ""),
          collapse="\n")
  }
})
col_points <- "#7f577492"
col_dark   <- "#5f4354"
```

```{r, echo=FALSE}
donkey <- read.csv("http://www.massey.ac.nz/~jcmarsha/227215/data/donkey.csv")
mod <- lm(Bodywt ~ Heartgirth, data=donkey)
```

## Learning outcomes

- Assumptions.

- Adding more covariates.

- The $F$-test.

- Models with factor variables.

# Assumptions

## Assumptions of linear models

**L**inearity

 - Residuals don't depend on $x$.

**I**ndependence

 - Residuals don't depend on each other.

**N**ormality

 - Residuals are distributed normally.

**E**qual variance

 - Residuals have constant variance.

## Checking the assumptions

- We can check the linear model assumptions by producing **diagnostic plots** of the model.

- Using these, we can assess linearity, normality, and equal variance, but we usually can't assess independence without using other information from the data (such as how it was collected).

- In addition, we can also see whether there are outliers in the data that have undue **influence** on our model fit.

- In RStudio, we can use `plot(model)` to get diagnostic plots.

## Example: Donkeys

```{r, echo=FALSE, fig.align='center', fig.width=8, fig.height=5}
par(mfrow=c(2,2), mar=c(4,2,2,2))
plot(mod, pch=19, col="#00000040", xaxt="n")
```

## Diagnostic plots: Residuals versus fits

```{r, results='asis', echo=FALSE, fig.height=6, fig.width=4.5, out.extra=''}
# hack-hack
is_pdf <- rmarkdown::all_output_formats("lecture2.Rmd")[1] == "beamer_presentation"
start_table <- function(is_pdf) {
  if (is_pdf) {
    cat("\\begin{minipage}{5cm}")
  } else { # HTML
    cat("<table class='container'><tr>")
    cat("<td style='width:500px'>")
  }
}
next_column <- function(is_pdf) {
  if (is_pdf) {
    cat("\\end{minipage}\n")
#    cat("\\begin{minipage}{5cm}")
  } else {
    cat("</td>")
    cat("<td style='vertical-align:top'>")
  }
}
end_table <- function(is_pdf) {
  if (is_pdf) {
    cat("")
#    cat("\\end{minipage}")
  } else {
    cat("</td>")
    cat("</tr></table>")
  }
}
start_table(is_pdf)
par(omi=c(2,0,0,0), mar=rep(0,4))
plot(mod, which=1, pch=19, col="#00000040", xaxt="n")
next_column(is_pdf)
```

- Allows us to assess **linearity**.

- There should be no curve.

- Also allows assessing **equal variance**.

- The points shouldn't fan out.

- When either of these fail, a log transform of $y$ and/or $x$ often straightens things out.

```{r, results='asis', echo=FALSE, out.extra=''}
end_table(is_pdf)
```

## Residuals versus fits

<iframe src="http://it056230.massey.ac.nz:8080/apps/227215/shiny/linearity/" style="border: none"></iframe>

## Diagnostic plots: Normal Q-Q plot

```{r, results='asis', echo=FALSE, fig.height=6, fig.width=4.5, out.extra=''}
start_table(is_pdf)
par(omi=c(2,0,0,0), mar=rep(0,4))
plot(mod, which=2, pch=19, col="#00000040", xaxt="n")
next_column(is_pdf)
```
- Allows us to assess **normality**.

- Ideally the points will lie on the straight line.

- A slight S shaped curve is OK.

- Central limit theorem means this can generally be ignored.

```{r, results='asis', echo=FALSE, out.extra=''}
end_table(is_pdf)
```

## Diagnostic plots: Scale-location plot

```{r, results='asis', echo=FALSE, fig.height=6, fig.width=4.5, out.extra=''}
start_table(is_pdf)
par(omi=c(2,0,0,0), mar=rep(0,4))
plot(mod, which=3, pch=19, col="#00000040", xaxt="n")
next_column(is_pdf)
```
- Allows us to assess **equal variance**.

- Ideally should not be increasing or decreasing.

- Residuals versus fit often tells you this just as well.

```{r, results='asis', echo=FALSE, out.extra=''}
end_table(is_pdf)
```

## Diagnostic plots: Residuals versus Leverage

```{r, results='asis', echo=FALSE, fig.height=6, fig.width=4.5, out.extra=''}
start_table(is_pdf)
par(omi=c(2,0,0,0), mar=rep(0,4))
plot(mod, which=5, pch=19, col="#00000040", xaxt="n")
next_column(is_pdf)
```
- Allows us to look for **influential outliers**.

- Points should ideally be inside red bands (Cook's distance) at 0.5.

- Points outside Cook's distance of 1 have excessive influence.

```{r, results='asis', echo=FALSE, out.extra=''}
end_table(is_pdf)
```

## Influential outliers

- Points can be outliers in two ways:

    - They can have extreme $x$ values compared to the rest of the data. Such points are said to have high **leverage**.
    
    - They can have extreme $y$ values, given their $x$ value (i.e. a large residual.)

- Points have large **influence** if they exhibit both these properties.

- **Cook's distance** is a measure of influence. Cook's distance larger than 1 means the points have large influence on the model fit. Removing these points may change the model quite a bit.

## Influential outliers

<iframe src="http://it056230.massey.ac.nz:8080/apps/227215/shiny/influence/" style="border: none"></iframe>

## Summary

- The residuals vs fitted plot should be looked at first. If you see curvature or increasing scatter from left to right, try a log transform and re-fit the model.

- Next look at residuals vs leverage. If you have influential outliers then look at the effect of removing those observations.

# The Linear model|Multiple covariates

## Relationships: Donkeys

```{r, echo=FALSE, fig.align="center", fig.width=8, fig.height=5}
donkey <- read.csv("http://www.massey.ac.nz/~jcmarsha/227215/data/donkey.csv")
par(mfrow=c(2,2), mar=c(4,4,1,1))
plot(Bodywt ~ Heartgirth, data=donkey, col=col_points, pch=19)
plot(Bodywt ~ Umbgirth, data=donkey, col=col_points, pch=19)
plot(Bodywt ~ Length, data=donkey, col=col_points, pch=19)
plot(Bodywt ~ Height, data=donkey, col=col_points, pch=19)
```

## Relationships: Donkeys

- There are reasonably strong increasing relationships between body weight and hearth girth, umbilical girth, length and height.

- It makes sense to use all of these variables to predict the body weight.

- That way we're likely to explain more of the variation in body weight.

- Predictions will be more precise (smaller confidence and prediction intervals).

## Recall: The linear model

- The linear model estimates the mean of the response variable for given values of a covariate
$$
\mathsf{mean}(y) = \alpha + \beta x
$$
- The model is fit by finding least squares estimates for $\alpha$ and $\beta$.

- The model assumptions are defined in terms of the residuals.

- When the assumptions hold we can take our model from the sample and generalise to the population.

## Multiple covariates

- Adding other covariates is as simple as adding another term to the linear model equation.
$$
\mathsf{mean}(y) = \alpha + \beta_1 x_1 + \beta_2 x_2 + \cdots
$$
- We find least squares estimates for $\alpha$, $\beta_1, \beta_2, \ldots$ in the same way, by minimising the residual sum of squares.

- As the model assumptions are defined in terms of the residuals, the complexity of the model equation doesn't change things much.

- We get additional lines in our output for each covariate we add to the model.

## Matrix version of the linear model

We can formulate the linear model in terms of a **matrix equation**
$$
Y = X \beta + \epsilon
$$
where $Y$ is a vector of the dependent variable measurements, $X$ is the **design matrix** of covariate values, $\beta$ is a vector of coefficients (to be estimated) and $\epsilon$ is a vector of residuals. The form $X$ takes is
$$
X = \left[\begin{array}{c c c c}
1 & x_{11} & \cdots & x_{p1}\\
1 & x_{12} & \cdots & x_{p2}\\
\vdots & \vdots & \ddots & \vdots\\
1 & x_{1n} & \cdots & x_{pn}
\end{array}\right]
$$
where $p$ is the number of covariates, and $n$ is the number of observations.

## Testing multiple covariates

- We don't want to just answer the question **"Which of the variables are important?"**

- Rather, we want to answer **"Which of the variables are important *after accounting for the other variables*?"**

- This is equivalent to: **"Which of the $\beta$'s are non-zero?"**

## Summary output for multiple covariates {.smaller .fragile}

```{r, echo=FALSE}
mod <- lm(Bodywt ~ Heartgirth + Umbgirth + Length + Height, data=donkey)
rs <- round(summary(mod)$r.squared,3)
fv <- round(summary(mod)$fstatistic[1],1)
summary(mod)
```

## Summary output for multiple covariates

- Heart girth, umbilical girth and length are all important after accounting for other covariates.

- Height is not important after accounting for the other covariates.

- **This doesn't mean that height isn't important for body weight!**

## But wait!

```{r}
summary(lm(Bodywt ~ Height, data=donkey))
```

## Summary output for multiple covariates

- Heart girth, umbilical girth and length are all important after accounting for other covariates.

- Height is not important after accounting for the other covariates.

- The $R^2$ for the model is `r rs`, so the model is explaining `r rs*100`% of the variation in body weight.

- What is the overall P-value telling us?

# The F-test

## The F-test

- One of the first things we want to know is whether anything at all in our model is useful.

- The hypothesis to test is **"Are any of the $\beta$'s non-zero?"**

- We can do this by comparing the variation in the residuals to the total variation in $y$.

- We use the **F-statistic** for this.
$$
F = \frac{(\sigma^2_\mathsf{total} - \sigma^2_\mathsf{res})/p}{\sigma^2_\mathsf{res}/(n-p-1)}
$$
where $p$ is the number of covariates, and $n$ the number of observations.

## The F-test
$$
F = \frac{(\sigma^2_\mathsf{total} - \sigma^2_\mathsf{res})/p}{\sigma^2_\mathsf{res}/(n-p-1)}
$$

- On the top we have variance explained by the model, and on bottom variance left over in residuals.

- The ratio of these, $F$ will increase as we explain more variation in the model.

- Under the hypothesis that the model doesn't explain anything we'd expect $F=0$.

- We can use the **F-distribution** to figure out how likely the $F$ from our sample
would arise by chance, giving us the P-value.

- We use the P-value for our decision about the model. $F$ is just the way the maths gets us
there (like $t$ in the $t$-test).

## The F-statistic in summary output

```
Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) -222.33956    8.01411 -27.744  < 2e-16 ***
Heartgirth     1.76223    0.12784  13.784  < 2e-16 ***
Umbgirth       0.37418    0.06726   5.563 5.01e-08 ***
Length         0.87580    0.11531   7.595 2.41e-13 ***
Height         0.25407    0.13749   1.848   0.0654 .  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 9.476 on 380 degrees of freedom
Multiple R-squared:  0.8519,	Adjusted R-squared:  0.8503 
### <b>
F-statistic: 546.4 on 4 and 380 DF,  p-value: < 2.2e-16
### </b>
```

An F value of `r fv` is really unlikely to arise by chance, so our model is telling us something about body weight.

# Linear models with factors

## Linear models with factors

- The linear model equation is describing the **mean** of the outcome variable given the value of the covariates.

- So we can use a linear model to estimate a mean just by not including any covariates, e.g. by using the model equation
$$
\mathsf{mean}(y) = \alpha
$$

- When we have a factor, or categorical covariate, the linear model will estimate the mean within each group.

## Factors with two levels

- For a factor with two levels, we can do this by introducing an **indicator variable** $z$ as a covariate, where
$$
z = \left\{
\begin{array}{ll}
0 & \textrm{for observations in level 1}\\
1 & \textrm{for observations in level 2}
\end{array}\right.
$$
- The equation is then
$$
\mathsf{mean}(y) = \alpha + \beta z
$$

## Factors with two levels

- The equation is then
$$
\mathsf{mean}(y) = \alpha + \beta z
$$
- For observations in level 1 we know $z=0$, so the equation gives
$$
\mathsf{mean}(y) = \alpha
$$
- For observations in level 2 we know $z=1$, so the equation gives
$$
\mathsf{mean}(y) = \alpha + \beta
$$
- Thus, $\beta$ represents the **difference** between level 1 and 2.

- Testing for a difference between groups is then just testing whether $\beta = 0$.

## Example: Donkeys {.smaller}

```{r}
mod <- lm(Bodywt ~ Sex, data=donkey)
summary(mod)
```

## Example: Donkeys

```{r, echo=FALSE}
co <- summary(lm(Bodywt ~ Sex, data=donkey))$coefficients
```
- The intercept is the mean body weight of female donkeys (`r round(co[1,1],1)` kg).

- The coefficient labelled `SexMale` is the difference between females and males (males `r round(co[2,1],2)` kg heavier).

- This difference isn't significant (P=`r round(co[2,4],3)`).

- This is **exactly the same** as a two sample t-test where we assume the variance in each group is the same.

## Example: Donkeys

```{r}
t.test(Bodywt ~ Sex, data=donkey, var.equal=TRUE)
```

<div class="centered">
**Linear model gives same results!**
</div>

## What about three levels?

- For two levels we introduced an indicator variable that was 1 when we were in level 2 and 0 otherwise.

- For three levels we introduce two indicator variables:

    - $z_2 = 1$ if the observation is in level 2, $z_2 = 0$ otherwise.

    - $z_3 = 1$ if the observation is in level 3, $z_3 = 0$ otherwise.

- The linear model is then
$$
\mathsf{mean}(y) = \alpha + \beta_2 z_{2} + \beta_3 z_{3}
$$
where $\beta_2$ is the difference between level 1 and level 2, and $\beta_3$ is the difference between level 1 and level 3.

## What about three levels?

- The linear model is then
$$
\mathsf{mean}(y) = \alpha + \beta_2 z_{2} + \beta_3 z_{3}
$$

- The primary hypothesis to test is "Is there any difference between the three levels?".

- This is the same as "Are any of the $\beta$'s non-zero?"

- The F-test!

## Example: Petrels

```{r, echo=FALSE, fig.width=9.5}
petrels <- read.csv("http://www.massey.ac.nz/~jcmarsha/227212/data/petrels.csv")
petrels$Area <- as.factor(petrels$Area)
petrels$Sex <- as.factor(petrels$Sex)
par(mar=c(5,5,1,0), cex.axis=1.5, cex.lab=1.5)
plot(R.Wing.Lth ~ Area, data=petrels, ylab="Right wing length (cm)")
```


## Example: Petrels {.smaller .fragile}

```{r, echo=FALSE}
mod <- lm(R.Wing.Lth ~ Area, data=petrels)
summary(mod)
```

## Example: Petrels

- The overall F-test's P-value suggests average right wing length differs between areas.

- Summary table suggests that Area 5 has largest birds, having 4.14cm larger ring wing lengths compared to those in Area 1.

- Birds in Area 6 are smallest, having 9.27 cm smaller right wing lengths compared to those in Area 1.

- We can get confidence intervals for the mean using the predict function

## Example: Petrels
```{r}
new_data <- data.frame(Area=factor(1:6))
predict(mod, new_data, interval="confidence")
```

## Visualising the differences

```{r, eval=FALSE}
library(visreg)
visreg(mod)
```

```{r, echo=FALSE, fig.width=9.5}
library(visreg)
par(mar=c(4,4,0,0))
visreg(mod)
```
