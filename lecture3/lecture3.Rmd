---
title: "Linear models 3"
author: "Jonathan Marshall"
date: ''
output:
  ioslides_presentation:
    css: jm.css
    highlight: tango
    widescreen: yes
  beamer_presentation:
    highlight: tango
    includes:
      in_header: ../header.tex
    keep_tex: yes
---

```{r setup, echo=FALSE}
library(knitr)
opts_chunk$set(dev.args=list(bg='transparent'), comment="")
#print(knit_hooks$get('output'))
knit_hooks$set(output=function (x, options) {
#  cat("OPTIONS=", names(options), "\n")
  cache_path <- unlist(strsplit(options$cache.path, '/'))
#  cat("Cache_path length=", length(cache_path), "\n")
  out_format <- cache_path[length(cache_path)]
  if (out_format == "html") {
    knitr:::escape_html(x)
    x = paste(x, collapse = "\\n")
    sprintf("<div class=\"%s\"><pre class=\"knitr %s\">%s</pre></div>\\n", 
        'output', tolower(options$engine), x)
  } else {
    paste(c("\\begin{ROutput}",
            sub("\n$", "", x),
            "\\end{ROutput}",
            ""),
          collapse="\n")
  }
})
col_points <- "#7f577492"
col_dark   <- "#5f4354"
```
## Learning outcomes

- Review

- Multiple factors

- Interactions

# Review

## Review

The linear model estimates the **mean** of the outcome given the covariates.

- the mean body weight for a given heart girth.

- the mean body weight given heart girth, umbilical girth, length and sex.

- the mean right wing length for different areas.

It also gives uncertainties, i.e. confidence intervals.

## Review

- For factor (grouping) variables, it estimates a separate mean for each level.

- The first level (baselines) goes into the intercept.

- Each other level is represented by the difference to the baseline.

- The overall F-test tells us whether the grouping variable is important.


## The F-statistic for factors

$$
F = \frac{(\sigma^2_\mathsf{total} - \sigma^2_\mathsf{res})/p}{\sigma^2_\mathsf{res}/(n-p-1)}
$$

- The numerator is variation explained by the model. In this case this is variation between groups, as the model is just giving a different mean to each group.

- The denominator is residual variation, which in this case is variation within groups (residuals are values minus fit, which is the group mean).

- Thus F is a ratio of between-group variation to within-group variation.

## Example: Petrels

```{r, echo=FALSE, fig.width=9.5}
petrels <- read.csv("http://www.massey.ac.nz/~jcmarsha/227212/data/petrels.csv")
petrels$Area <- as.factor(petrels$Area)
petrels$Sex <- as.factor(petrels$Sex)
par(mar=c(5,5,1,0), cex.axis=1.5, cex.lab=1.5)
plot(R.Wing.Lth ~ Area, data=petrels, ylab="Right wing length (mm)")
```

## Example: Petrels {.fragile .smaller}

```{r, echo=FALSE}
mod <- lm(R.Wing.Lth ~ Area, data=petrels)
summary(mod)
```

## Example: Petrels

- The overall F-test's P-value suggests average right wing length differs between areas.

- Summary table suggests that Area 5 has largest birds, having 4.14mm larger ring wing lengths compared to those in Area 1.

- Birds in Area 6 are smallest, having 9.27 mm smaller right wing lengths compared to those in Area 1.

- We can get confidence intervals for the mean using the predict function

## Example: Petrels
```{r}
new_data <- data.frame(Area=factor(1:6))
predict(mod, new_data, interval="confidence")
```

## Visualising the differences

```{r, eval=FALSE}
library(visreg)
visreg(mod)
```

```{r, echo=FALSE, fig.width=9.5}
library(visreg)
par(mar=c(4,4,0,0))
visreg(mod)
```

# Multiple factors

## What if we have more than one factor?

- We include a block of indicator variables for each factor.

- The first level, or **baseline** of each of the factors is what contributes to the intercept.

- Subsequent levels are treated as differences compared to the baseline level.

- The overall F-test and P-value for the model is testing whether any of the factors in the model are important.

- We need another way to compute P-values for each factor separately.

## Summary output for two factors {.fragile .smaller}

```{r, echo=FALSE}
mod.2 <- lm(R.Wing.Lth ~ Sex + Area, data=petrels)
summary(mod.2)
```

## The anova function{.fragile}

```{r}
mod2 <- lm(R.Wing.Lth ~ Sex + Area, data=petrels)
anova(mod2)
```

- Each row in the anova table corresponds to a factor (or numeric covariate).

- P-values thus can be read out.

## Process of assessing a factor covariate

- Fit linear model.

- Do `anova()` to check the factor is important (small P).

- If it is important, look at the `summary()` to see how the levels differ (or `visreg()` to visualise them).

- Multiple testing problem on the latter, which is why we do the overall test first.

- **Order can be important in the anova table.**

- Each row's P-values are adjusting for previous rows.

## Example: Order matters

```{r}
anova(lm(R.Wing.Lth ~ Sex + Area, data=petrels))
```

## Example: Order matters

```{r}
anova(lm(R.Wing.Lth ~ Area + Sex, data=petrels))
```

## ANOVA table

- Each row represents a single variable (factor or numeric).

- Each row adjusted for what is above it (but not below).

- Order can be important but probably tells you something if it changes things.

- If a factor is important, look at summary table to see which groups differ (or visualise model).

## Petrels: Summary table {.fragile .smaller}

```{r, echo=FALSE}
summary(mod2)
co <- round(coef(mod2),2)
```

## Petrels: Conclusion

- From ANOVA table, both `Area` and `Sex` are important to the right wing length.

- From summary table, we see that males have `r co["SexMale"]`mm larger right wing length than females **after accounting for differences between areas**.

```{r, echo=FALSE}
mf = tapply(petrels$R.Wing.Lth, petrels$Sex, mean, na.rm=TRUE)
mf = round(mf[2]-mf[1],2)
ar = tapply(petrels$R.Wing.Lth, petrels$Area, mean, na.rm=TRUE)
ar = round(ar[5]-ar[1],2)
```

- This doesn't mean that male birds have a `r co["SexMale"]`mm larger right wing length than females.

- In fact
    ```{r}
    tapply(petrels$R.Wing.Lth, petrels$Sex, mean, na.rm=TRUE)
    ```
    they're `r mf`mm larger across all areas.

## Petrels: Conclusion

- Similarly, birds in area 5 have `r co["Area5"]`mm larger right wing length than those in area 1 **after accounting for differences in sex**.

- Again,
    ```{r}
    tapply(petrels$R.Wing.Lth, petrels$Area, mean, na.rm=TRUE)
    ```
    they're `r ar`mm larger across both sexes.

## Summary

- Always look at the `anova` table first, particularly if you have factors.

- Look at the `summary` table next, and interpret the estimated coefficients there for the significant variables from the `anova` table.

- Use `visreg` to visualise the effects - easier to see effect sizes/uncertainty.

- Remember that 'not significant' does not mean 'not important'.

# Interactions

## Interactions

- Our current models have been assuming that each effect is **additive**.

- e.g. the difference in the right wing length between the sexes is common across all areas, and similarly the area effects are common for males and females.

- This is often not the case.

## Example: Hypothetical drugs

- Suppose we have some medical condition and we have a numerical measure describing how good/bad a patient is.

- Suppose further that we have two drugs A and B to treat that condition, and that we have the option of combining them.

- Then there are 4 possible treatments: No drugs, just drug A, just drug B, or both drugs.

- Our current linear models, would assume that we can add the effect of drug A and B together for the "both drugs" case.

## Example: Hypothetical drugs

The model equation would be
$$
\mathsf{mean}(y) = \alpha + \beta_A z_A + \beta_B z_B
$$
where $z_A$ and $z_B$ are indicators for whether drug A or B are included, giving

Treatment     Mean
------------  ----
Neither drug  $\mathsf{mean}(y)=\alpha$
Just drug A   $\mathsf{mean}(y)=\alpha + \beta_A$
Just drug B   $\mathsf{mean}(y)=\alpha + \beta_B$
Both drugs    $\mathsf{mean}(y)=\alpha + \beta_A + \beta_B$

## Example: Hypothetical drugs

- For the combined drugs treatment, we have $\mathsf{mean}(y)=\alpha + \beta_A + \beta_B$

- We're thus assuming that the individual benefit of each drug add together when combined.

- But, it might be that the combined effect of the drugs gives is smaller or larger than the sum of individual effects.

- We need a bit more flexibility in the model.

## Example: Hypothetical drugs

<iframe src="http://it056230.massey.ac.nz:8080/apps/227215/shiny/twoway/" style="border: none"></iframe>

## Interactions: Hypothetical drugs

We add another indicator variable $z_{AB}$ which is 1 when both drugs are in use.
$$
\mathsf{mean}(y) = \alpha + \beta_A z_A + \beta_B z_B + \beta_{AB} z_{AB}
$$

*Mathematical aside: the new variable is the product of the existing ones, as $z_{AB}=1$ only if both $z_A=1$ and $z_B=1$. $z_{A} \times z_{B}$ has this property.*

The means for the neither drug or single drug treatments are still the same, as $z_{AB}=0$ in those cases. The mean in the both drugs treatment is the only one that changes to
$$
\mathsf{mean}(y) = \alpha + \beta_A + \beta_B + \beta_{AB}.
$$
Thus, $\beta_{AB}$ can be used to measure whether or not the effects of the two drugs are additive or not.

## Interactions: Hypothetical drugs

- $\beta_{AB}$ can be used to measure whether or not the effects of the two drugs are additive or not.

- An alternate way to think about it is that the effect of drug B may differ depending on whether the patient is already given drug A.

- e.g. the effect for drug B may be to increase the diagnostic measurement by 5 units in the absence of other treatment, but if the patient is on drug A as well, then the effect of drug B may be less pronounced.

## Example: Hypothetical drugs

<iframe src="http://it056230.massey.ac.nz:8080/apps/227215/shiny/twoway/" style="border: none"></iframe>

## Interactions in RStudio

```{r}
mod3 = lm(R.Wing.Lth ~ Sex + Area + Sex:Area, data=petrels)
anova(mod3)
```
As the interaction here is between factors, it should be evaluated using the ANOVA table.

## Interactions in RStudio

```{r, echo=FALSE}
pval=round(anova(mod3)[,5],3)
names(pval)=rownames(anova(mod3))
```
- From the `anova` table we see the interaction term is not significant (P=`r pval["Sex:Area"]`).

- Thus, there is no evidence in the data to suggest that the comparative size of male and female petrels differ between areas.

## Summary table of interactions{.fragile .smaller}

```{r, echo=FALSE}
summary(mod3)
co3=round(coef(mod3),2)
```

## Summary table of interactions

- While the interaction term isn't significant (thus all the interaction coefficients could be zero in the population) let's assess what each row represents.

- The Intercept contains the first levels of each factor, so females in area 1 have on average `r co3["(Intercept)"]`mm right wing lengths.

- The `SexMale` term is the difference to the baseline, so this describes how males in area 1 differ. They have right wing lengths `r co3["SexMale"]`mm larger.

- The `Area2` term is the difference to the baseline, so this describes how females in area 2 differ from those in area 1. Their right wing lengths are `r -co3["Area2"]`mm smaller.

- The `SexMale:Area2` is the differential effect of males in area 2 over and above the `SexMale` and `Area2` effects. Compared with females in area 1, males in area 2 are $`r co3["SexMale"]`+`r co3["Area2"]`+`r co3["SexMale:Area2"]`=`r sum(co3[c("SexMale","Area2","SexMale:Area2")])`$mm larger.

## Visualising models using visreg

- When you have interactions, visualise the effect of one variable within the levels of another.

- Allows you to see how effects interact.

- Better way to present results from the model than summary table.

```{r, echo=TRUE, eval=FALSE}
library(visreg)
visreg(mod3, "Area", by="Sex")
visreg(mod3, "Area", by="Sex", overlay=TRUE)
```

## visreg: Model without interaction

```{r, echo=FALSE, fig.width=9.5, fig.height=5.5}
par(mar=c(4,4,0,0))
visreg(mod2, "Area", by="Sex")
```

## visreg: Model with interaction

```{r, echo=FALSE, fig.width=9.5, fig.height=5.5}
par(mar=c(4,4,0,0))
visreg(mod3, "Area", by="Sex")
```

## visreg: without interaction (`overlay=TRUE`)

```{r, echo=FALSE, fig.width=9.5, fig.height=5.5, warning=FALSE}
par(mar=c(4,4,1,0))
visreg(mod2, "Area", by="Sex", overlay=TRUE)
```

## visreg: with interaction (`overlay=TRUE`)

```{r, echo=FALSE, fig.width=9.5, fig.height=5.5, warning=FALSE}
par(mar=c(4,4,1,0))
visreg(mod3, "Area", by="Sex", overlay=TRUE)
```

## Other types of interaction

Consider
$$
\mathsf{mean}(y) = \alpha + \beta_1 x + \beta_2 z
$$
where $x$ is a numeric variable and $z$ is an indicator variable for the 2nd level of a factor variable.

Then if we're in the first level of the factor variable, $z=0$ so the equation is
$$
\mathsf{mean}(y) = \alpha + \beta_1 x
$$
If we're in the second level, $z=1$ so that
$$
\mathsf{mean}(y) = \alpha + \beta_1 x + \beta_2 = (\alpha + \beta_2) + \beta_1 x.
$$

The model fit is thus two parallel lines, separated by $\beta_2$.

## Example: Calfweights

```{r, echo=FALSE, fig.width=8.5, fig.height=5.5, fig.align='center'}
calf <- read.csv("http://www.massey.ac.nz/~jcmarsha/227215/data/calfweight.csv")
par(mar=c(4,4,1,2))
plot(Weight ~ Age, data=calf, col=Breed, pch=as.numeric(Treatment), xlab="Age (days)", ylab="Weight (kg)")
legend('topleft', legend=levels(calf$Breed), col=1:3, pch=1, title="Breed")
legend('bottomright', legend=levels(calf$Treatment), pch=1:2, title="Treatment")
```

## Example: Calf weights

```{r}
c1 = lm(Weight ~ BirthWeight + Breed + Treatment + Age, data=calf)
anova(c1)
```

## Example: Calf weights

```{r, fig.width=8, fig.height=5.5, fig.align='center'}
visreg(c1, "Age", by="Treatment", overlay=TRUE)
```

## Example: Calf weights

```{r, fig.width=8, fig.height=5.5, fig.align='center'}
visreg(c1, "Age", by="Breed", overlay=TRUE)
```

## Interaction of numeric variables and factors

We could add another term using the product of the numeric variable $x$ and the indicator for the second level of the factor $z$
$$
\mathsf{mean}(y) = \alpha + \beta_1 x + \beta_2 z + \beta_3 x z.
$$

Then if we're in the first level of the factor variable, $z=0$ so the equation is
$$
\mathsf{mean}(y) = \alpha + \beta_1 x
$$
whereas if we're in the second level, $z=1$ so that
$$
\mathsf{mean}(y) = \alpha + \beta_1 x + \beta_2 + \beta_3 x = (\alpha + \beta_2) + (\beta_1 + \beta_3) x.
$$
The model fit is thus two separate lines.

## Example: Calfweights

```{r, echo=FALSE, fig.width=8, fig.height=5.5, fig.align='center'}
par(mar=c(4,4,1,2))
plot(Weight ~ Age, data=calf, col=Breed, pch=as.numeric(Treatment), xlab="Age (days)", ylab="Weight (kg)")
legend('topleft', legend=levels(calf$Breed), col=1:3, pch=1, title="Breed")
legend('bottomright', legend=levels(calf$Treatment), pch=1:2, title="Treatment")
```

## Example: Calf weights

```{r}
c2 <- lm(Weight ~ BirthWeight + Breed + Treatment + Age +
           Age:Treatment + Age:Breed, data=calf)
anova(c2)
```

## Calf weight summary{.fragile .smaller}

```{r, echo=FALSE}
summary(c2)
```

## Calf weights{.smaller}

```{r, fig.width=8, fig.height=5.5, fig.align='center'}
visreg(c2, "Age", by="Breed", overlay=TRUE)
```

## Calf weights{.smaller}

```{r, fig.width=8, fig.height=5.5, fig.align='center'}
visreg(c2, "Age", by="Treatment", overlay=TRUE)
```

## Diagnostics

```{r, echo=FALSE, fig.width=8, fig.height=5.5, fig.align='center'}
par(mfrow=c(2,2), mar=c(4,4,2,2))
plot(c2, add.smooth=FALSE)
```

## Better model

```{r}
c3 <- lm(log(Weight) ~ log(BirthWeight) + Breed + Treatment + Age + 
         Age:Treatment + Age:Breed, data=calf)
anova(c3)
```

## Better model: Diagnostics

```{r, echo=FALSE, fig.width=8, fig.height=5.5, fig.align='center'}
par(mfrow=c(2,2), mar=c(4,4,2,2))
plot(c3, add.smooth=FALSE)
```

## Better model: Fit{.smaller}

```{r, fig.width=8, fig.height=5.5, fig.align='center'}
visreg(c3, "Age", by="Breed", trans=exp, partial=TRUE, overlay=TRUE, ylab="Weight")
```

## Better model: Fit{.smaller}

```{r, fig.width=8, fig.height=5.5, fig.align='center'}
visreg(c3, "Age", by="Treatment", trans=exp, partial=TRUE, overlay=TRUE, ylab="Weight")
```

## Another problem though!

- We know here that we have multiple calves measured through time.

- Residuals from the same calf likely more similar to each other than to those
of other calves.

- The residuals are unlikely to be independent.

- In this case can check by plotting the residuals versus the
calf identifier.

## Another problem?

```{r, fig.width=9, fig.height=4.5, fig.align='center', echo=2}
par(mar=c(4,5,0,1))
boxplot(resid(c3) ~ Calf, data=calf, xlab="Calf", ylab="Residual")
```

## Solution: See lab 4!

```{r, eval=FALSE}
library(nlme)
mixed = lme(log(Weight) ~ log(BirthWeight) + Breed + Treatment + Age + 
            Breed:Age + Treatment:Age, random=~1|Calf, data=calf)
boxplot(resid(mixed) ~ Calf, data=calf)
```

## Solution: See lab 4!

```{r, echo=FALSE, fig.width=9, fig.height=5, fig.align='center'}
library(nlme)
mixed = lme(log(Weight) ~ log(BirthWeight) + Breed + Treatment + Age + 
            Breed:Age + Treatment:Age, random=~1|Calf, data=calf)
par(mar=c(4,4,0,1))
boxplot(resid(mixed) ~ Calf, data=calf, xlab="Calf", ylab="Residual")
```

## Learning outcomes

- Review

- Multiple factors

- Interactions
