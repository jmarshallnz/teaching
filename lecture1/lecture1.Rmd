---
title: "Linear models"
author: "Jonathan Marshall"
date: "18 June 2015"
output: 
  ioslides_presentation: 
    highlight: tango
    widescreen: true
    css: jm.css
---

```{r setup, echo=FALSE}
library(knitr)
opts_chunk$set(dev.args=list(bg='transparent'))
col_points <- "#7f577492"
col_dark   <- "#5f4354"
```
## Learning outcomes

- The linear model.
- Using RStudio to fit linear models.
- Prediction.
- Assumptions of the linear model.

# Example: Donkeys

## Example: 385 Moroccan donkeys.

Variable   Description
--------   -----------
Sex        Sex (Female/Male)
Age        Age (years)
Bodywt     Weight (kg)
Heartgirth Girth at the heart (cm)
Umbgirth   Girth at the umbilicus (cm)
Length     Length from elbow to hind (cm)
Height     Height to the withers (cm)

## Relationships: Donkeys

```{r, echo=FALSE, fig.align="center", fig.width=8, fig.height=5}
donkey <- read.csv("http://www.massey.ac.nz/~jcmarsha/227215/data/donkey.csv")
par(mfrow=c(2,2), mar=c(4,4,1,1))
plot(Bodywt ~ Heartgirth, data=donkey, col=col_points, pch=19)
plot(Bodywt ~ Umbgirth, data=donkey, col=col_points, pch=19)
plot(Bodywt ~ Length, data=donkey, col=col_points, pch=19)
plot(Bodywt ~ Height, data=donkey, col=col_points, pch=19)
```

## Relationships: Donkeys

```{r, echo=FALSE, fig.align="center"}
plot(Bodywt ~ Sex, data=donkey, horizontal=TRUE)
```

## Relationships: Donkeys

- There are reasonably strong increasing relationships between body weight and hearth girth, umbilical girth, length and height.
- There doesn't seem much difference between the sexes.
- The *strongest* relationship is with heart girth. **Why?**
- We'll look at **modelling** this relationship using a straight line.

# The Linear Model

## Linear modelling

- Try to explain the variation in one measurement using other measurements.
- Key is to model the mean of one variable conditional on the values of other variables.
- Once we have a model, we can use that model for **prediction** or for quantifying the extent of the relationship.

## Definitions

- The **response**, or **dependent variable**, is the numeric variable we wish to model (the $y$ variable).
- The **explanatory** variable(s), or **predictors**, **covariates**, or **independent variables** are the $x$ variables
we use to explain the response. They needn't be numeric.
- The **regression line** or **linear model** is the way we relate $y$ to the explanatory variables $x$.
- For the simple case of a single numeric predictor, the equation is
$$
\mathsf{mean}(y) = \alpha + \beta x
$$
where $\alpha$ is the **intercept** or **baseline**, and $\beta$ is the **slope**, or **gradient**.

## Definitions

```{r, echo=FALSE, fig.align="center"}
# generate some data for a generic plot
set.seed(2015)
x <- rnorm(100, 3, 1)
y <- 0.5 + 0.5*x + rnorm(100, 0, 0.3)
par(mar=c(2,1,1,1), cex=1.5)
plot(y ~ x, col="#00000020", xlim=c(0, max(x)+0.2), ylim=c(0, max(y)+0.2), pch=19, xlab="", ylab="", xaxt="n", yaxt="n", xaxs="i", yaxs="i")
line <- lm(y ~ x) 
abline(line, lwd=2)
xv <- c(1,5)
yv <- predict(line, data.frame(x=xv))
lines(xv, rep(yv[1], 2), lty="dotted")
lines(rep(xv[2], 2), yv, lty="dotted")
text(mean(xv), yv[1], "run", adj=c(0.5,1.2), col="grey30")
text(xv[2], mean(yv), "rise", adj=c(-0.2,0.5), col="grey30")
mtext(expression(alpha), side=2, line=0.5, at = coef(line)[1], las=1, cex=1.5)
text(4, yv[1]-0.5, expression(beta==over(rise,run)))
axis(1, at=0, line=0)
```

## Sample versus Population

- Combined, the $\alpha$ and $\beta$ are known as **coefficients** of the linear model in the population. Linear modelling is the process of fitting this model to data, where we **estimate** these coefficients.

- We only ever have a sample of data, thus our estimates are sample statistics, which we hope will be representative of
the true parameters $\alpha$ and $\beta$ in the population.

## Estimation of parameters

- For each point $x_i$ we have the corresponding estimated value $\hat{y}_i = \alpha + \beta x_i$.
- The difference between the real $y_i$ and fitted $\hat{y}_i$ is the **residual** $\epsilon_i = y_i - \hat{y}_i$.
- We find the optimal $\alpha$ and $\beta$ by minimising the variance of the residuals.
- This is called **least squares** estimation as the formula for variance contains a sum of squares
$$
\begin{align}
\mathsf{var}(\epsilon_i) &= \frac{1}{n}\sum_{i=1}^n (\epsilon_i - 0)^2\\
&= \frac{1}{n}\sum_{i=1}^n (y_i - (\alpha + \beta x_i))^2
\end{align}
$$

## Estimation of parameters

- We minimise the **residual sum of squares**
$$
SS_\mathsf{res} = \sum_{i=1}^n (y_i - (\alpha + \beta x_i))^2
$$
- The values of $\alpha$ and $\beta$, found using calculus, are **least squares estimates**.
- We often put hats on them ($\hat{\alpha}$, $\hat{\beta}$) to help us remember that they're sample statistics and not the true values from the population.
- We'll add some assumptions later so we can our estimates to the population parameters.

## Estimation of parameters

<iframe src="http://it056230.massey.ac.nz:8080/227215/shiny/leastsquares/" style="border: none"></iframe>

# Linear models in RStudio

## Linear models in RStudio

```{r, eval=FALSE}
mod <- lm(Bodywt ~ Heartgirth, data=donkey)
summary(mod)
```

## Linear models in RStudio

```{r, comment="", echo=FALSE}
mod <- lm(Bodywt ~ Heartgirth, data=donkey)
summary(mod)
```

## Linear models in RStudio

```{r, echo=FALSE}
s <- summary(mod)$coefficients
```
- The `Estimate` column gives us the values of our coefficients.
    - The estimate for `Heartgirth` means a 1 unit increase in heart girth corresponds to a `r round(coef(mod)[2],2)` unit increase in mean body weight.

- The `Std. Error` column is the standard error of our estimates. We can use this to find confidence intervals.
    - For heart girth, we're 95% confident that the coefficient of heartgirth is within
$$
`r round(s[2,1],2)` \pm 2 \times `r round(s[2,2],2)` = (`r round(s[2,1]-2*s[2,2],2)`, `r round(s[2,1]+2*s[2,2],2)`).
$$

## Linear models in RStudio

- The `t value` column is the test statistic for the hypothesis that the coefficient is 0.
- The `Pr(>|t|)` column is the P-value for this hypothesis test.
- This is equivalent to asking "Does body weight depend on heart girth?"
- In this case, the P-value is very small, so there is very little chance that we'd observe an estimate as large as `r round(s[2,1],2)` if there was no relationship between body weight and heart girth in the population.
- Our conclusion would be that body weight does depend on heart girth.

## Linear models in RStudio

- The `Multiple R-squared` value is the proportion of variation in body weight explained by the model (i.e. explained by heart girth).
$$
R^2 = \frac{\sigma_\mathsf{Y}^2 - \sigma_\mathsf{res}^2}{\sigma_\mathsf{Y}^2} = \frac{SS_\mathsf{total} - SS_\mathsf{res}}{SS_\mathsf{total}}
$$
- The `p-value` for the model (last line) is testing whether **anything** in the model helps explain body weight.
    - Same as P-value for heart girth in this case, as only thing in the model.

## Prediction

- We can predict the mean of $y$ for any given value of $x$ by just substituting values into the estimated linear model equation.
- We can utilise the standard errors of our estimates to give uncertainty for the predicted average.
    - This is a **confidence interval**.
- We can combine this with the variance of the residuals to give a prediction of the spread of individual values around the average.
    - This is a **prediction interval**.

## Prediction

```{r, echo=FALSE, fig.align='center', fig.width=7, fig.height=5}
par(mar=rep(0,4), cex=1.5)
plot(y ~ x, col="#00000050", xlim=c(min(x)-0.25, max(x)+0.25), ylim=c(min(x)-0.25, max(y)+0.25), pch=19, xlab="", ylab="", xaxt="n", yaxt="n", xaxs="i", yaxs="i")
xv <- seq(0,6,0.01)
yv_c <- predict(line, data.frame(x=xv), interval="confidence")
yv_p <- predict(line, data.frame(x=xv), interval="prediction")
polygon(c(xv,rev(xv)),c(yv_p[,2], rev(yv_p[,3])), col="#00000020", border=NA)
polygon(c(xv,rev(xv)),c(yv_c[,2], rev(yv_c[,3])), col="#00000040", border=NA)
abline(coef(line), lwd=2)
legend("bottomright", fill=c("#00000060", "#00000020"), legend=c("Confidence interval", "Prediction interval"), bty="n")
```

## Prediction in RStudio

```{r, comment=""}
new.data <- data.frame(Heartgirth=c(100,120))
predict(mod, new.data, interval="confidence")
predict(mod, new.data, interval="prediction")
```

# Assumptions

## Assumptions of linear models

**L**inearity

 - Residuals don't depend on $x$.

**I**ndependence

 - Residuals don't depend on each other.

**N**ormality

 - Residuals are distributed normally.

**E**qual variance

 - Residuals have constant variance.

## {.bigequation .flexbox .vcenter}

$$
\epsilon_i \mathop{\sim}\limits_\mathsf{iid} \mathsf{Normal}(0, \sigma^2)
$$

## Sampling distribution of our estimates

- As long as our assumptions hold then $\hat\alpha$ and $\hat\beta$ both have $t$&nbsp;distributions.

- We won't bother with the equations for them, other than to note:

    - The estimates $\hat\alpha$ and $\hat\beta$ are **unbiased**, i.e.
    $$
    \mathsf{mean}(\hat\alpha) = \alpha, \qquad \mathsf{mean}(\hat\beta) = \beta
    $$
    - The standard errors of $\hat\alpha$ and $\hat\beta$ decrease with $n$ in the same way that the standard error of the sample mean decreases with $n$.
    - We can test hypotheses about $\alpha$ and $\beta$ using these distributions.

## Checking the assumptions

- We can check the linear model assumptions by producing **diagnostic plots** of the model.

- Using these, we can assess linearity, normality, and equal variance, but we usually can't assess independence without using other information from the data (such as how it was collected).

- In addition, we can also see whether there are outliers in the data that have undue **influence** on our model fit.

- In RStudio, we can use `plot(model)` to get diagnostic plots.

## Diagnostic plots: Residuals versus fits

<div class="columns-2">
```{r, echo=FALSE, fig.height=4, fig.width=4.5}
par(mar=rep(0,4))
plot(mod, which=1, pch=19, col="#00000040")
```

- Allows us to assess **linearity**.
- There should be no curve.
- Also allows assessing **equal variance**.
- The points shouldn't fan out.
- When either of these fail, a log transform of $y$ and/or $x$ often straightens things out.
</div>

## Diagnostic plots: Normal Q-Q plot

<div class="columns-2">
```{r, echo=FALSE, fig.height=4, fig.width=4.5}
par(mar=rep(0,4))
plot(mod, which=2, pch=19, col="#00000040")
```

- Allows us to assess **normality**.
- Ideally the points will lie on the straight line.
- A slight S shaped curve is OK.
</div>

## Diagnostic plots: Scale-location plot

<div class="columns-2">
```{r, echo=FALSE, fig.height=4, fig.width=4.5}
par(mar=rep(0,4))
plot(mod, which=3, pch=19, col="#00000040")
```

- Allows us to assess **equal variance**.
- Ideally should not be increasing or decreasing.
- Residuals versus fit often tells you this just as well.
</div>

## Diagnostic plots: Residuals versus Leverage

<div class="columns-2">
```{r, echo=FALSE, fig.height=4, fig.width=4.5}
par(mar=rep(0,4))
plot(mod, which=5, pch=19, col="#00000040")
```

- Allows us to look for **influential outliers**.

- Points should ideally be inside red bands (Cook's distance) at 0.5.

- Points outside Cook's distance of 1 have excessive influence.
</div>

## Influential outliers

- Points can be outliers in two ways:
    - They can have extreme $x$ values compared to the rest of the data. Such points are said to have high **leverage**.
    - They can have extreme $y$ values, given their $x$ value (i.e. a large residual.)

- Points have large **influence** if they exhibit both these properties.

- **Cook's distance** is a measure of influence. Cook's distance larger than 1 means the points have large influence on the model fit. Removing these points may change the model quite a bit.

## Influential outliers

<iframe src="http://it056230.massey.ac.nz:8080/227215/shiny/influence/" style="border: none"></iframe>

## When assumptions don't hold

- **Linearity**. Estimates will be *biased*. Fix this before looking at other assumptions.

- **Independence**. Estimates will be unbiased, but standard errors will be wrong. P-values may be underestimated.

- **Normality**. Estimates will be unbiased, but coefficients may not have the $t$&nbsp;distribution we expect. As long as sample size is reasonable and shape not too extreme the **central limit theorem** comes to the rescue.

- **Equal variance**. Estimates will be unbiased, but standard errors will be wrong. P-values may be underestimated.

<div class="centered">
*A log transformation often fixes linearity or unequal variance.*
</div>
